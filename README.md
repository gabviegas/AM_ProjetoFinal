<p align="center">

# FunG: Modelagem de distribui√ß√£o de fam√≠lias de Fungos

A proposta do projeto √© elaborar um modelo preditivo com dados sobre esp√©cies de fungos nativos do Brasil para determinar a fam√≠lia predominante de fungos em uma determinada regi√£o. Esse tipo de an√°lise ]pode ser √∫til para fins comerciais, dado que fungos apresentam uma gama de aplica√ß√µes medicinais, culin√°rias, entre outras. Al√©m disso, pode auxiliar em estudos de biogeografia, fornecendo insights sobre como as condi√ß√µes do ambiente culmina na ocorr√™ncia e predomin√¢ncia dessas esp√©cies. 

## Come√ßando

O projeto visa aplicar t√©cnicas de aprendizado de m√°quina para prever a ordem de uma esp√©cie de fungo nativa do Brasil com base em sua localiza√ß√£o. Baseando-se em caracter√≠sticas como municipality, fam√≠lia, longitude m√≠nima e latitude m√≠nima observadas, o modelo busca identificar padr√µes ecol√≥gicos e geogr√°ficos que influenciam a distribui√ß√£o das ordens desses fungos. 


## üî® Etapas do projeto

- [ ] Acesso e tratamento de dados.
- [ ] Realizar splitting dos dados e testes.
- [ ] Otimiza√ß√£o com Optuna.
- [ ] Modelo Baseline Classificador.
- [ ] Matriz de confus√£o.
- [ ] Implementar K-NN vizinhos.
- [ ] Implementar floresta aleat√≥ria.
- [ ] Teste com Naive Bayes.
- [ ] Comentar resultados.
      
##  Implanta√ß√£o

Para a implanta√ß√£o desse projeto foi utilizada a interface do Jupyter Notebook para elaborar o c√≥digo fonte, em conjunto com as ferramentas pandas, matplolib, numpy, seaborn, sklearn para carregamento, tratamento e exibi√ß√£o dos dados utilizados no projeto. 

## üõ†Ô∏è Constru√≠do com

As seguintes ferramentas foram utilizadas para a elabora√ß√£o do projeto:

### Optuna:
√â um m√≥dulo para resolver problemas envolvendo otimiza√ß√£o com par√¢metros num√©ricos e categ√≥ricos que proprociona buscas mais estrat√©gico do que a busca aleat√≥ria e mais eficiente de que a busca em grade. Pontanto, √© uma estrutura de software de otimiza√ß√£o autom√°tica de hiperpar√¢metros de um modelo de aprendizado de m√°quina que tamb√©m se integra ao  acompanhamento e monitoramento de modelo e avalia√ß√£o.

### Baseline:
Tamb√©m conhecido como Dummy, linha de base ou modelo fict√≠cio. No contexto de ci√™ncia de dados e aprendizado de m√°quina, o baseline pode ser visto como uma solu√ß√£o de refer√™ncia que define um ponto de partida. Ele foi projetado para ser simples e de f√°cil implementa√ß√£o, como um modelo que sempre prev√™ a m√©dia (para problemas de regress√£o) ou a classe mais frequente (para problemas de classifica√ß√£o). 

### K-NN Vizinhos: 
O funcionamento de um modelo preditivo induzido por este algoritmo √© simples: ao receber um certo exemplo x, o algoritmo checa a dist√¢ncia deste exemplo  com rela√ß√£o aos exemplos que ele ja conhece(vizinhos). O valor predito pelo modelo ser√° a m√©dia dos alvos dos k vizinhos mais pr√≥ximos do exemplo de entrada( k vizinhos com menor dist√¢ncia).[1].

### Floresta aleat√≥ria: 
Combina fun√ß√µes de predi√ß√£o aproximadamente n√£o viesadas fazendo uma classifica√ß√£o, treinado com covari√°veis dadas pelas predi√ß√µes dos modelos a serem combinados. √â formada por diversas √°rvores de decis√£o onde o processo de construir cada das √°rvores de decis√£o desta floresta envolve amostragem dos exemplos e de atributos.[2].

### M√©tricas de classifica√ß√£o:
Este m√≥dulo oferece v√°rias fun√ß√µes  para medir o desempenho de modelos de classifica√ß√£o, incluindo fun√ß√µes de perda e pontua√ß√µes. Algumas m√©tricas podem exigir estimativas de probabilidade da classe positiva, valores de confian√ßa ou valores de decis√µes bin√°rias. A maioria das implementa√ß√µes permite que cada amostra forne√ßa uma contribui√ß√£o ponderada √† pontua√ß√£o geral, por meio do par√¢metro.

* Recall:
√â uma m√©trica usada para medir a propor√ß√£o de positivos verdadeiros que s√£o corretamente idendtificados. Sendo intuitivamenete a capacidade do classificador de encontrar todas as amostras positivas, √© muito √∫til em situa√ß√µes em que a dete√ß√£o de casos positivos √© crucial e a ocorr√™ncia de falsos negativos √© indejada.

### Naive Bayes (NB):
Os classificadores Naive Bayes s√£o um conjunto de algoritmos probabil√≠sticos que realizam predi√ß√µes com base no Teorema de Bayes. S√£o chamados "naive" (ing√™nuo) porque assumem uma rela√ß√£o de independ√™ncia condicional entre as features que analisa.

$$
P(C | X) = \frac{P(C) P(X | C)}{P(X)}
$$

O Teorema de Bayes descreve a probabilidade de ocorr√™ncia de um evento C ocorrer sabendo que um evento X ocorreu. √â uma ferramenta extremamente poderosa, que proporciona a possibilidade de atualizar uma probabilidade inicial matematicamente, tendo aplica√ß√µes extremamante difundidas em diversas √°reas, como aprendizado de m√°quina, sa√∫de, ou mesmo a vida cotidiana. Para a equa√ß√£o acima, temos os termos:

* $P(C|X):$ probabilidade de C sabendo que X ocorreu.
* $P(X|C):$ probabilidade de X sabendo que C ocorreu.
* $P(C):$ probabilidade inicial de C, o que se sabia de C antes de X ocorrer.
* $P(X):$ √© a probabilidade total de X.

#### Aplica√ß√£o ao contexto

Tomando ainda como refer√™ncia a equa√ß√£o acima, considerando que $X = {X_1, X_2, X_3, ..., X_N}$, sendo X o conjunto das features que contribuem para os dados de treino e teste e que C representa uma classe que estamos tentando prever, o Teorema de Bayes √© aplicado no NB de forma que a cada feature analisada em rela√ß√£o a uma classe tenha probabilidade de ocorrer independente de outra, o que resulta em:

$$
P(X|C) = P(X1|C) * P(X2|C)...P(X_N|C)
$$

Assim, a probabilidade que representa a rela√ß√£o da manisfeta√ß√£o das features dada um determinada classe pode ser representada pelo produto das probabilidades individuais de cada feature. o Teorema de Bayes aplicado ao contexto desse tipo de previs√£o fica:

$$
P(C | X) = \frac{P(X1|C) * P(X2|C)...P(X_N|C)}{P(X)}
$$

Assim, a probabilidade de uma classe C ocorrer se as features $X_1, X_2, ..., X_N$ se manifestarem pode ser calculada pelo teorema. √â importante lembrar que essa probabilidade √© a atualiza√ß√£o de uma probailidade inicial P(C). Como temos 4 classes, esse processo √© realizado para as 4. A classe que possuir a probabilidade mais alta ser√° a indicada como correspondente ao conjunto de features utilizadas no c√°lculo.

Foi mencionado que Naive Bayes √© um conjunto de algoritmos probabil√≠sticos. Como op√ß√µes de algoritmos para implementa√ß√£o dessa l√≥gica, ter√≠amos o `MultinomialNB`, altamente recomendado para a classifica√ß√£o de textos, `BernoulliNB`, recomendado para dados que apresentem distribui√ß√µes Bernoulli multivariadas, `CategoricalNB` para dados distribu√≠dos categoricamente e `GaussianNaiveBayes`, que assume que as vari√°veis analisadas est√£o seguindo uma distribui√ß√£o Gaussiana. 
Os algoritmos que ser√£o implementados nesse projeto ser√£o os de Gaussian Naive Bayes e Complementar Naive Bayes (uma adapta√ß√£o para conjunto de dados desbalanceados). 



## Refer√™ncias
CASSAR, Daniel. "ATP-203 2.1- Aprendizado de m√°quina, k-NN e m√©tricas.ipynb" [Material de sala de aula]. Aprendizado de M√°quina, 07 de agosto de 2024, Ilum - Escola de Ci√™ncia.

IZBICKI, Rafael; DOS SANTOS, Tiago Mendon√ßa. Aprendizado de m√°quina: uma abordagem estat√≠stica. 2020. Dispon√≠vel em: http://www.rizbicki.ufscar.br/ame/.
